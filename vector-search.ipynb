{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader #vai converter o texto em um formato que o Lang Chain possa trabalhar\n",
    "from langchain_text_splitters import CharacterTextSplitter #vai dividir o texto em diferentes chunks de caracteres (há diferentes formas de fazer isso. Vale à pena pesquisar)\n",
    "from langchain_openai import OpenAIEmbeddings #vai converter o texto em embeddings (vetores) que o Lang Chain possa trabalhar utilizando modelos do OpenAI\n",
    "from langchain_chroma import Chroma #vaii guardar os vectors em vectors databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos agora utilizar as chaves de API do OpenAI. Parta isso vamos utilizar a a biblioteca dotenv\n",
    "# com .env\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "books = pd.read_csv('books_cleaned.csv')\n",
    "display(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(books[\"tag_description\"])\n",
    "\n",
    "#A tag é uma ótima forma de indentificar o nome do livro após utilizar a descrição para treinar o modelo, pois o que vamos ter de retorno na busca de vetores \n",
    "# é a descrição com a tag do livro. Utilizar a descrição para filtrar pelo livro é muito devagar, por isso utilizar a tag é uma melhor opçção\n",
    "\n",
    "#vamos então extrair as descrições e as tags dos livros\n",
    "\n",
    "books[\"tag_description\"].to_csv(\"books_tag_description.csv\", index=False, sep = \"\\n\", header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = TextLoader(\"books_tag_description.csv\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=0, chunk_overlap = 0, separator = \"\\n\") #vamos utilizar chunk_size = 0 para garantir que ele vai fazer a separação baseado no separator \n",
    "# e não em chunks de caracteres e utilizar chunk_overlap = 0 para garantir que ele não vai haver overlap entre as descrições.\n",
    "documents = text_splitter.split_documents(raw_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checando abaixo percebe-se que funcionou corretamente a separação das descrições\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos agora criar os embeddings para cada descrição e salvá-los em um banco de dados de vetores\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "db_books = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=huggingface_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#podemos agora dar um query para o banco de dados de vetores para encontrar os livros mais similares a essa query\n",
    "\n",
    "query = \"A book to teach children about nature\"\n",
    "docs = db_books.similarity_search(query, k=10)\n",
    "display(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mas nós queremos que o usuário possa pesquisar por um livro e não por uma descrição\n",
    "\n",
    "#vamos extrair então os isbn13 di oage_content e utilizar para filtrar o nome do livro com o valor de docs\n",
    "books[books[\"isbn13\"] == int(docs[0].page_content.split()[0].strip())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_book_by_description(query, books_df, db_books, k=10):\n",
    "    docs = db_books.similarity_search(query, k=k)\n",
    "    all_codes = [int(doc.page_content.split()[0].strip('\"')) for doc in docs]\n",
    "    return books_df[books_df[\"isbn13\"].isin(all_codes)]\n",
    "\n",
    "find_book_by_description(\"A book about games\", books, db_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot classification\n",
    "\n",
    "Agora vamos trabalhar com zero-shot classification, onde conseguimos utilizar modelos LLM para prever o tipo de um livro a partir da sua descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora quer nós temos um modelos de busca de livros baseado em descrições, podemos procurar formas de refinar essa busca.\n",
    "\n",
    "\n",
    "#Vamos fazer uma classificação utilizando somente modelos de ficção e não-ficção. Então vamos simplificar as categorias\n",
    "# Nós vamos então utilizar de text-classification para diminuir o número de categorias nos nossos dados e utilizar isso\n",
    "# como um potencial filtro no BookRecommender\n",
    "\n",
    "# A utilização dos LLMs para esse fim é chamado de zero-shot classification, que praticamente associada uma descrição ou texto a uma categoria\n",
    "\n",
    "#Primeiro vamos definir as cotegorias que gostaríamos de utilizar para classificar os livros: fiction e non-fiction\n",
    "\n",
    "#Vamos pegar as categorias com 50 ou mais livros\n",
    "\n",
    "categories_count = books[[\"categories\"]].value_counts().sort_values(ascending=False)\n",
    "categories_count = categories_count[categories_count >= 50]\n",
    "display(categories_count)\n",
    "\n",
    "#E utilizar elas pra dividir os livros em fiction e non-fiction\n",
    "\n",
    "category_mapping = {\n",
    "    \"Fiction\" : \"Fiction\",\n",
    "    \"Juvenile Fiction\" : \"Fiction\",\n",
    "    \"Biography & Autobiography\" : \"Nonfiction\",\n",
    "    \"History\" : \"Nonfiction\",\n",
    "    \"Literary Criticism\": \"Nonfiction\",\n",
    "    \"Philosophy\": \"Nonfiction\",\n",
    "    \"Religion\": \"Nonfiction\",\n",
    "    \"Comics & Graphic Novels\": \"Fiction\",\n",
    "    \"Drama\": \"Fiction\",\n",
    "    \"Juvenile Nonfiction\": \"Nonfiction\",\n",
    "    \"Science\": \"Nonfiction\",\n",
    "    \"Poetry\": \"Fiction\"\n",
    "}\n",
    "\n",
    "books[\"simple_categories\"] = books[\"categories\"].map(category_mapping)\n",
    "\n",
    "#Percebe-se que temos um bom conjunto de livros que podem ser utilizados na classificação com LLMS\n",
    "\n",
    "display(books[~(books[\"simple_categories\"].isna())])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos utilizar então o modelo da hugging face para nosso zero-shot classification\n",
    "\n",
    "# from transformers import pipeline\n",
    "\n",
    "# fictions_catregories = [\"Fiction\", \"Nonfiction\"]\n",
    "\n",
    "# pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0) #device = 0 para utilizar a GPU, caso não tenha GPU, pode-se utilizar device=-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = books.loc[books[\"simple_categories\"] == \"Fiction\", \"description\"].reset_index(drop=True)[0]\n",
    "# pipe(sequence, fictions_catregories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos utilizar então o modelo da hugging face para nosso zero-shot classification\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "from transformers import pipeline\n",
    "class BookClassification:\n",
    "    def __init__(self, model_name=\"facebook/bart-large-mnli\", device=0, batch_size=16):\n",
    "        self.model = model_name\n",
    "        self.device = device\n",
    "        self.categories = [\"Fiction\", \"Nonfiction\"]\n",
    "        self.pipe = self._load_model()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            return pipeline(\"zero-shot-classification\", model=self.model, device=self.device)\n",
    "        except torch.OutOfMemoryError as e:\n",
    "            print(e)\n",
    "            return pipeline(\"zero-shot-classification\", model=self.model, device=-1)        \n",
    "\n",
    "    def classify_batch(self, sequences):\n",
    "        results = self.pipe(sequences, candidate_labels=self.categories, batch_size=self.batch_size)\n",
    "        if isinstance(results, dict):\n",
    "            results = [results]\n",
    "        return [r[\"labels\"][np.argmax(r[\"scores\"])] for r in results]\n",
    "    \n",
    "\n",
    "book_classifier = BookClassification(model_name=\"facebook/bart-large-mnli\", device=0, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E então vamso criar duas listas com os valores reais para livros de ficção e não-ficção\n",
    "# e utilizar o modelo para classificar os livros\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Agrupar descrições\n",
    "fiction_desc = books.loc[books[\"simple_categories\"] == \"Fiction\", \"description\"].reset_index(drop=True)[:300]\n",
    "nonfiction_desc = books.loc[books[\"simple_categories\"] == \"Nonfiction\", \"description\"].reset_index(drop=True)[:300]\n",
    "\n",
    "# Unir tudo\n",
    "all_descriptions = list(fiction_desc) + list(nonfiction_desc)\n",
    "actual_cats = [\"Fiction\"] * 300 + [\"Nonfiction\"] * 300\n",
    "\n",
    "# Fazer predições em batch\n",
    "predicted_cats = []\n",
    "batch_size = 4\n",
    "for i in tqdm(range(0, len(all_descriptions), batch_size)):\n",
    "    batch = all_descriptions[i:i+batch_size]\n",
    "    predicted = book_classifier.classify_batch(batch)\n",
    "    predicted_cats.extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agora podemos comparar os valores reais com os valores previstos e verificar a acurácia do modelo\n",
    "\n",
    "prediction_df = pd.DataFrame({\"actual_categories\": actual_cats, \"predicted_categories\": predicted_cats})\n",
    "prediction_df[\"correct_prection\"] = np.where(prediction_df[\"actual_categories\"] == prediction_df[\"predicted_categories\"],1,0)\n",
    "prediction_df[\"correct_prection\"].sum() / len(prediction_df) * 100\n",
    "\n",
    "# o modelo teve uma acurácia em volta de 75%, o que é um bom resultado para um modelo de zero-shot classification que não foi designado especificamente para essas categorias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cats = books.loc[books[\"simple_categories\"].isna(), [\"isbn13\", \"description\"]].reset_index(drop=True)\n",
    "\n",
    "predicted_cats_missing = []\n",
    "isbn13_missing = []\n",
    "\n",
    "batch_size = 4\n",
    "for i in tqdm(range(0, len(missing_cats), batch_size)):\n",
    "    batch = missing_cats[\"description\"][i:i+batch_size].tolist()\n",
    "    predicted = book_classifier.classify_batch(batch)\n",
    "    predicted_cats_missing.extend(predicted)\n",
    "    isbn13_missing.extend(missing_cats[\"isbn13\"][i:i+batch_size].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
